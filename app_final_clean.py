import streamlit as st
import pandas as pd
import requests
import time
import folium
from konlpy.tag import Okt
from sklearn.cluster import KMeans
import numpy as np
from streamlit_folium import st_folium

# --------------------------
# âœ… ìœ„í—˜ì–´ ì‚¬ì „ ì •ì˜
# --------------------------
danger_words = [
    'ì¹¨ìˆ˜', 'ì§€í•˜ì°¨ë„', 'í•˜ì²œ ë²”ëŒ', 'ëŒ€í”¼', 'í­ìš°', 'ì°¨ ì ê¹€', 'ë„ë¡œ ì ê¹€', 'ì •ì „',
    'ë‹¨ìˆ˜', 'ê°‡í˜', 'í†µì œ', 'ì§€í•˜ì²  ì¹¨ìˆ˜', 'ë„ë¡œ ìœ ì‹¤', 'ì§€ì˜¥', 'ê³µí¬', 'í—¬ê²Œì´íŠ¸'
]

# --------------------------
# âœ… ìœ„í—˜ì–´ ì ìˆ˜ í•¨ìˆ˜
# --------------------------
def count_danger_words(text):
    return sum(1 for word in danger_words if word in text)

def danger_score_to_risk_level(n):
    if n == 0:
        return 1  # ì•ˆì „
    elif n == 1:
        return 2  # ì£¼ì˜
    else:
        return 3  # ìœ„í—˜

# --------------------------
# âœ… ì¢Œí‘œ ì¶”ì¶œ (Kakao API)
# --------------------------
KAKAO_API_KEY = "115286bcd7c3ab9e37176a29d08e25b7"

def get_lat_lng_kakao(ì£¼ì†Œ):
 URL = "https://dapi.kakao.com/v2/local/search/address.json "
 í—¤ë” = {"ê¶Œí•œ ë¶€ì—¬": f"KakaoAK {KAKAO_API_KEY}"}
 ë§¤ê°œë³€ìˆ˜ = {"query": ì£¼ì†Œ}
 ì‘ë‹µ = requests.get(url, í—¤ë”=headers, íŒŒëŒ=íŒŒëŒ)
 if response.status_code == 200ì´ê³  response.json ()['documents']:
 doc = ì‘ë‹µ.json ()['documents'][0]
 ë°˜í™˜ í”Œë¡œíŠ¸(doc[y']), í”Œë¡œíŠ¸(doc['x'])
 ì—†ìŒ, ì—†ìŒ ë°˜í™˜

# --------------------------
# âœ… ìŠ¤íŠ¸ë¦¼ë¼ì´íŠ¸ ì•± ì‹¤í–‰
# --------------------------
st.title("ğŸš¨ ì‹¤ì‹œê°„ íŠ¸ìœ— ê¸°ë°˜ ì¹¨ìˆ˜ ìœ„í—˜ ì§€ë„")

ì¿¼ë¦¬ = st.text_input (" ğŸ“ íŠ¸ìœ— í‚¤ì›Œë“œ ì…ë ¥", value="ì¹¨ìˆ˜ OR ì§€í•˜ì°¨ë„ OR ì •ì „")

if st.button("ğŸš€ íŠ¸ìœ— ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œì‘"):
 BEARER_TOKEN = "AAAAAAAAAAAAAAAAAAAAABd%2B0gEAAAAAyCgw6GhEuAK8j1ly0OMJr5lI43g%3DG0XXqIF44Ay5dvDqNWaa6Gq6MtgWtu77WNhge4pSJnbYAnPiHz"
 í—¤ë” = {"ê¶Œí•œ ë¶€ì—¬": f"ë² ì–´ëŸ¬ {BEARER_TOKEN}"}
 URL = "https://api.twitter.com/2/tweets/search/recent "
    params = {
        "query": f"({query}) lang:ko -is:retweet",
        "max_results": 10,
        "tweet.fields": "created_at,text"
    }

    response = requests.get(url, headers=headers, params=params)
    if response.status_code != 200:
        st.error("âŒ íŠ¸ìœ— ìˆ˜ì§‘ ì‹¤íŒ¨")
    else:
        tweets = response.json().get("data", [])
        locations, latlngs, texts, risks = [], [], [], []
        okt = Okt()

        for tweet in tweets:
            text = tweet['text']
            nouns = okt.nouns(text)
            filtered_nouns = [n for n in nouns if len(n) >= 2]

            for noun in filtered_nouns:
                lat, lng = get_lat_lng_kakao(noun)
                if lat and lng:
                    risk = danger_score_to_risk_level(count_danger_words(text))
                    locations.append((lat, lng, text, risk))
                    latlngs.append((lat, lng))
                    texts.append(text)
                    risks.append(risk)
                    break
                time.sleep(1.5)

 # ì§€ë„ ì‹œê°í™”
 m = í´ë¥¨.ì§€ë„(ìœ„ì¹˜=[37.5665, 126.9780], zoom_start=11)
 color_map = {1: 'ë…¹ìƒ‰', 2: 'orange', 3: 'ë¹¨ê°„ìƒ‰'}

 lat, lng, í…ìŠ¤íŠ¸, ìœ„ì¹˜ ìœ„í—˜:
 ì—½ë¡ì†Œ.ì„œí´ë§ˆì»¤(
 ìœ„ì¹˜=[lat, lng],
 ë°˜ì§€ë¦„=6,
 color=color_map[ìœ„í—˜],
 ì±„ì›€=ì°¸,
 ì±„ì›€ ë¶ˆíˆ¬ëª…ë„=0.7,
 íŒì—…=í´ë¥¨.íŒì—…(f"<b>ìœ„í—˜ë„: {ìœ„í—˜}//b><br>{í…ìŠ¤íŠ¸[:60]}..., max_width=300),
 íˆ´íŒ=f"ìœ„í—˜ë„ {ë¦¬ìŠ¤í¬}ë‹¨ê³„"
 ). add_to(m)

 ì„¸ì¸íŠ¸ ì„œë¸Œí—¤ë” ("ğŸ—ºï¸ ìœ„í—˜ ì§€ì—­ ì§€ë„")
 st_folium(m, ë„ˆë¹„=700)

        # ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
 df = pd.ë°ì´í„°í”„ë ˆì„({
 'ë‚´ìš©': í…ìŠ¤íŠ¸,
 'ìœ„í—˜ë„': ìœ„í—˜
        })

 ì„¸ì¸íŠ¸ ì„œë¸Œí—¤ë” ("ğŸ“‹ ìœ„í—˜ ë¶„ì„ ê²°ê³¼")
 st.dataframe(df)
