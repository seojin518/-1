import streamlit as st
import pandas as pd
import requests
import time
import folium
from konlpy.tag import Okt
from sklearn.cluster import KMeans
import numpy as np
from streamlit_folium import st_folium

# --------------------------
# âœ… ìœ„í—˜ì–´ ì‚¬ì „ ì •ì˜
# --------------------------
danger_words = [
    'ì¹¨ìˆ˜', 'ì§€í•˜ì°¨ë„', 'í•˜ì²œ ë²”ëŒ', 'ëŒ€í”¼', 'í­ìš°', 'ì°¨ ì ê¹€', 'ë„ë¡œ ì ê¹€', 'ì •ì „',
    'ë‹¨ìˆ˜', 'ê°‡í˜', 'í†µì œ', 'ì§€í•˜ì²  ì¹¨ìˆ˜', 'ë„ë¡œ ìœ ì‹¤', 'ì§€ì˜¥', 'ê³µí¬', 'í—¬ê²Œì´íŠ¸'
]

# --------------------------
# âœ… ìœ„í—˜ì–´ ì ìˆ˜ í•¨ìˆ˜
# --------------------------
def count_danger_words(text):
    return sum(1 for word in danger_words if word in text)

def danger_score_to_risk_level(n):
    if n == 0:
        return 1  # ì•ˆì „
    elif n == 1:
        return 2  # ì£¼ì˜
    else:
        return 3  # ìœ„í—˜

# --------------------------
# âœ… ì¢Œí‘œ ì¶”ì¶œ (Kakao API)
# --------------------------
KAKAO_API_KEY = "115286bcd7c3ab9e37176a29d08e25b7"

def get_lat_lng_kakao(ì£¼ì†Œ):
 URL = "https://dapi.kakao.com/v2/local/search/address.json "
 í—¤ë” = {"ê¶Œí•œ ë¶€ì—¬": f"KakaoAK {KAKAO_API_KEY}"}
 ë§¤ê°œë³€ìˆ˜ = {"query": ì£¼ì†Œ}
 ì‘ë‹µ = ìš”ì²­.get(url, í—¤ë”=headers, íŒŒëŒ=íŒŒëŒ)
 if response.status_code == 200 ì´ê³  response.json ()['documents']:
 ë¬¸ì„œ = ì‘ë‹µ.json ()['documents'][0]
 ë°˜í™˜ í”Œë¡œíŠ¸(doc['y']), í”Œë¡œíŠ¸(doc['x']) # ìœ„ë„, ê²½ë„
 ì—†ìŒ, ì—†ìŒ ë°˜í™˜


# --------------------------
# âœ… ìŠ¤íŠ¸ë¦¼ë¼ì´íŠ¸ ì•± ì‹¤í–‰
# --------------------------
st.title("ğŸš¨ ì‹¤ì‹œê°„ íŠ¸ìœ— ê¸°ë°˜ ì¹¨ìˆ˜ ìœ„í—˜ ì§€ë„")

ì¿¼ë¦¬ = st.text_input (" ğŸ“ íŠ¸ìœ— í‚¤ì›Œë“œ ì…ë ¥", value="ì¹¨ìˆ˜ OR ì§€í•˜ì°¨ë„ OR ì •ì „")

if st.button("ğŸš€ íŠ¸ìœ— ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œì‘"):
 BEARER_TOKEN = "AAAAAAAAAAAAAAAAAAAAABd%2B0gEAAAAAyCgw6GhEuAK8j1ly0OMJr5lI43g%3DG0XXQIF44Ay5dvDqNWaa6Gq6MtgWtu77WNhge4pSJnbYAnPiHz"
 í—¤ë” = {"ê¶Œí•œ ë¶€ì—¬": f"ë² ì–´ëŸ¬ {BEARER_TOKEN}"}
 URL = "https://api.twitter.com/2/tweets/search/recent "
 ë§¤ê°œë³€ìˆ˜ = {
 "query": f"{query} lang:ko -is:retweet",
 "max_results": 10,
 íŠ¸ìœ—.í•„ë“œ": "created_at,text"
 }

 ì‘ë‹µ = ìš”ì²­.get(url, í—¤ë”=headers, íŒŒëŒ=íŒŒëŒ)
 if response.status_code!= 200:
 st.error("âŒ íŠ¸ìœ— ìˆ˜ì§‘ ì‹¤íŒ¨")
 ê·¸ë ‡ì§€ ì•Šìœ¼ë©´:
 íŠ¸ìœ— = ì‘ë‹µ.json ()ì„ ê°€ì ¸ì˜µë‹ˆë‹¤ ("ë°ì´í„°", [])
 ìœ„ì¹˜, ì„ ë°˜, í…ìŠ¤íŠ¸, ìœ„í—˜ = [], [], [], []
 okt = okt ()

 íŠ¸ìœ—ì— ëŒ€í•œ íŠ¸ìœ—:
 í…ìŠ¤íŠ¸ = íŠ¸ìœ— ['í…ìŠ¤íŠ¸']
 ëª…ì‚¬ = okt.nouns(í…ìŠ¤íŠ¸)
 í•„í„°ë§ëœ_nouns = [ëª…ì‚¬ì—ì„œ n ì˜ ê²½ìš° ë Œ(n) >= 2]

 í•„í„°ë§ëœ_nounsì˜ ëª…ì‚¬ì— ëŒ€í•´:
 lat, lng = get_lat_lng_kakao(noun)
 ë¼íŠ¸ ë° LNG ì¸ ê²½ìš°:
 ìœ„í—˜ = danger_score_to_risk_level (count_danger_words(í…ìŠ¤íŠ¸))
 ìœ„ì¹˜.append((ë¼íŠ¸, LNG, í…ìŠ¤íŠ¸, ìœ„í—˜))
 latngs. append((lat, lng))
                    texts.append(text)
                    risks.append(risk)
                    break
                time.sleep(1.5)

 # ì§€ë„ ì‹œê°í™”
 m = í´ë¥¨.ì§€ë„(ìœ„ì¹˜=[37.5665, 126.9780], zoom_start=11)
 color_map = {1: 'ë…¹ìƒ‰', 2: 'orange', 3: 'ë¹¨ê°„ìƒ‰'}

 lat, lng, í…ìŠ¤íŠ¸, ìœ„ì¹˜ ìœ„í—˜:
 ì—½ë¡ì†Œ.ì„œí´ë§ˆì»¤(
 ìœ„ì¹˜=[lat, lng],
 ë°˜ì§€ë¦„=6,
 color=color_map[ìœ„í—˜],
 ì±„ì›€=ì°¸,
 ì±„ì›€ ë¶ˆíˆ¬ëª…ë„=0.7,
 íŒì—…=í´ë¥¨.íŒì—…(f"<b>ìœ„í—˜ë„: {ìœ„í—˜}//b><br>{í…ìŠ¤íŠ¸[:60]}..., max_width=300),
 íˆ´íŒ=f"ìœ„í—˜ë„ {ë¦¬ìŠ¤í¬}ë‹¨ê³„"
 ). add_to(m)

 ì„¸ì¸íŠ¸ ì„œë¸Œí—¤ë” ("ğŸ—ºï¸ ìœ„í—˜ ì§€ì—­ ì§€ë„")
 st_folium(m, ë„ˆë¹„=700)

        # ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
 df = pd.ë°ì´í„°í”„ë ˆì„({
 'ë‚´ìš©': í…ìŠ¤íŠ¸,
 'ìœ„í—˜ë„': ìœ„í—˜
        })

 ì„¸ì¸íŠ¸ ì„œë¸Œí—¤ë” ("ğŸ“‹ ìœ„í—˜ ë¶„ì„ ê²°ê³¼")
 st.dataframe(df)
