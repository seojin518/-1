
[import streamlit as st
import pandas as pd
import requests
import time
import folium
from konlpy.tag import Okt
from sklearn.cluster import KMeans
import numpy as np
from streamlit_folium import st_folium

# --------------------------
# âœ… ìœ„í—˜ì–´ ì‚¬ì „ ì •ì˜
# --------------------------
danger_words = ['ì¹¨ìˆ˜', 'ì§€í•˜ì°¨ë„', 'í•˜ì²œ ë²”ëŒ', 'ëŒ€í”¼', 'í­ìš°', 'ì°¨ ì ê¹€', 'ë„ë¡œ ì ê¹€', 'ì •ì „', 'ë‹¨ìˆ˜', 'ê°‡í˜', 'í†µì œ', 'ì§€í•˜ì²  ì¹¨ìˆ˜', 'ë„ë¡œ ìœ ì‹¤', 'ì§€ì˜¥', 'ê³µí¬', 'í—¬ê²Œì´íŠ¸']

# --------------------------
# âœ… ìœ„í—˜ì–´ ì ìˆ˜ í•¨ìˆ˜
# --------------------------
def count_danger_words(text):
    return sum(1 for word in danger_words if word in text)

def danger_score_to_risk_level(n):
    if n == 0:
        return 1
    elif n == 1:
        return 2
    else:
        return 3

# --------------------------
# âœ… ì¢Œí‘œ ì¶”ì¶œ (Kakao API)
# --------------------------
KAKAO_API_KEY = "115286bcd7c3ab9e37176a29d08e25b7"

def get_lat_lng_kakao(address):
    url = "https://dapi.kakao.com/v2/local/search/address.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {"query": address}
    response = requests.get(url, headers=headers, params=params)
    if response.status_code == 200 and response.json()['documents']:
        doc = response.json()['documents'][0]
        return float(doc['y']), float(doc['x'])
    return None, None

# --------------------------
# âœ… ì•± ì‹œì‘
# --------------------------
st.title("ğŸš¨ ì‹¤ì‹œê°„ íŠ¸ìœ— ê¸°ë°˜ ì¹¨ìˆ˜ ìœ„í—˜ ì§€ë„")

query = st.text_input("ğŸ“ íŠ¸ìœ— í‚¤ì›Œë“œ ì…ë ¥", value="ì¹¨ìˆ˜ OR ì§€í•˜ì°¨ë„ OR ì •ì „")

if st.button("ğŸš€ íŠ¸ìœ— ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œì‘"):
    BEARER_TOKEN = "AAAAAAAAAAAAAAAAAAAAABd%2B0gEAAAAAyCgw6GhEuAK8j1ly0OMJr5lI43g%3DG0XXqIF44Ay5dvDqNWaa6Gq6MtgWtu77WNhge4pSJnbYAnPiHzë ¥"
    headers = {"Authorization": f"Bearer {BEARER_TOKEN}"}
    url = "https://api.twitter.com/2/tweets/search/recent"
    params = {
        "query": f"({query}) lang:ko -is:retweet",
        "max_results": 10,
        "tweet.fields": "created_at,text"
    }

    response = requests.get(url, headers=headers, params=params)
    if response.status_code != 200:
        st.error("âŒ íŠ¸ìœ— ìˆ˜ì§‘ ì‹¤íŒ¨")
    else:
        tweets = response.json().get("data", [])
        locations, coords, texts, risks = [], [], [], []

        okt = Okt()
        for tweet in tweets:
            text = tweet['text']
            nouns = okt.nouns(text)
            filtered_nouns = [n for n in nouns if len(n) >= 2]
            for noun in filtered_nouns:
                lat, lng = get_lat_lng_kakao(noun)
                if lat:
                    risk = danger_score_to_risk_level(count_danger_words(text))
                    locations.append((lat, lng, text, risk))
                    coords.append((lat, lng))
                    texts.append(text)
                    risks.append(risk)
                    break
            time.sleep(1.5)

        # ì§€ë„ ì‹œê°í™”
        m = folium.Map(location=[37.5665, 126.9780], zoom_start=11)
        color_map = {1: 'green', 2: 'orange', 3: 'red'}

        for lat, lng, text, risk in locations:
            folium.CircleMarker(
                location=[lat, lng],
                radius=6,
                color=color_map[risk],
                fill=True,
                fill_opacity=0.7,
                popup=folium.Popup(f"<b>ìœ„í—˜ë„: {risk}</b><br>{text[:60]}...", max_width=300),
                tooltip=f"ìœ„í—˜ë„ {risk}ë‹¨ê³„"
            ).add_to(m)

        st.subheader("ğŸ—ºï¸ ìœ„í—˜ ì§€ì—­ ì§€ë„")
        st_folium(m, width=700)

        df = pd.DataFrame({
            'ë‚´ìš©': texts,
            'ìœ„í—˜ë„': risks
        })
        st.subheader("ğŸ“‹ ìœ„í—˜ ë¶„ì„ ê²°ê³¼")
        st.dataframe(df)
]
